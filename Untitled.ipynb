{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6bae9c-c11f-40c0-bd9c-53f152b2753b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_excel(excel_fd, csv_fd):\n",
    "\n",
    "    df = pd.read_excel(excel_fd)\n",
    "    df.to_csv(csv_fd, index=False)\n",
    "    \n",
    "    return pd.read_csv(csv_fd) \n",
    "\n",
    "    # return np.genfromtxt(csv_fd, delimiter=',', skip_header=1)\n",
    "\n",
    "data = load_excel(\"Dados_ML.xlsx\",\"dados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766623d2-7162-41ae-a794-8419ad4cbddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cd_instituic</th>\n",
       "      <th>cd_curso</th>\n",
       "      <th>sexo</th>\n",
       "      <th>ord_ingresso</th>\n",
       "      <th>cd_tip_est_sec</th>\n",
       "      <th>cd_hab_ant</th>\n",
       "      <th>cd_regime</th>\n",
       "      <th>idadeReal</th>\n",
       "      <th>trabalhadorEstudante</th>\n",
       "      <th>...</th>\n",
       "      <th>internacional</th>\n",
       "      <th>maiores 23</th>\n",
       "      <th>outros</th>\n",
       "      <th>1geracao</th>\n",
       "      <th>nota10-12</th>\n",
       "      <th>nota12-14</th>\n",
       "      <th>nota14-16</th>\n",
       "      <th>nota16-18</th>\n",
       "      <th>nota18-20</th>\n",
       "      <th>Abandono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>792</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  cd_instituic  cd_curso  sexo  ord_ingresso  cd_tip_est_sec  \\\n",
       "0         227             1         1     1             1               1   \n",
       "1         228             0        20     0             1               1   \n",
       "2         229             1         9     1             1               1   \n",
       "3         792             1         1     1             1               1   \n",
       "4         793             1         1     1             1               1   \n",
       "\n",
       "   cd_hab_ant  cd_regime  idadeReal  trabalhadorEstudante  ...  internacional  \\\n",
       "0           1          0         26                     0  ...              0   \n",
       "1           1          2         48                     1  ...              0   \n",
       "2           1          2         49                     1  ...              0   \n",
       "3          21          0         36                     1  ...              0   \n",
       "4           1          0         42                     1  ...              0   \n",
       "\n",
       "   maiores 23  outros  1geracao  nota10-12  nota12-14  nota14-16  nota16-18  \\\n",
       "0           0       1         1          1          0          0          0   \n",
       "1           0       1         0          1          0          0          0   \n",
       "2           0       1         0          1          0          0          0   \n",
       "3           0       1         0          1          0          0          0   \n",
       "4           0       1         1          1          0          0          0   \n",
       "\n",
       "   nota18-20  Abandono  \n",
       "0          0         0  \n",
       "1          0         0  \n",
       "2          0         0  \n",
       "3          0         0  \n",
       "4          0         0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "673124c3-de6a-4a16-bb7d-87348d119abd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sexo\n",
       "0    2718\n",
       "1    2693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sexo.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d5c709-9ce2-4820-81bd-d9996bac28aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cd_instituic\n",
       "0    2868\n",
       "1    1230\n",
       "2     749\n",
       "3     564\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cd_instituic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341276d6-233c-454e-916e-a4922585e5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abandono\n",
       "0    4771\n",
       "1     640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abandono.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8826ae03-76d4-4ce4-92ce-af97c0cdd54a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idadeReal\n",
       "18    1678\n",
       "19     925\n",
       "17     561\n",
       "20     520\n",
       "21     277\n",
       "22     194\n",
       "23     139\n",
       "24     116\n",
       "25     105\n",
       "26      95\n",
       "27      72\n",
       "28      69\n",
       "29      53\n",
       "36      45\n",
       "32      43\n",
       "38      38\n",
       "30      35\n",
       "31      35\n",
       "34      34\n",
       "37      32\n",
       "40      32\n",
       "33      31\n",
       "44      29\n",
       "35      28\n",
       "43      25\n",
       "42      24\n",
       "39      23\n",
       "46      21\n",
       "48      20\n",
       "41      18\n",
       "45      16\n",
       "49      16\n",
       "47      12\n",
       "51      11\n",
       "53      10\n",
       "50       9\n",
       "54       4\n",
       "58       3\n",
       "59       3\n",
       "62       2\n",
       "52       2\n",
       "55       1\n",
       "69       1\n",
       "16       1\n",
       "56       1\n",
       "63       1\n",
       "1        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.idadeReal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d496ce9a-dad0-42e7-a610-29ee5423299d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maiores 23\n",
       "0    4852\n",
       "1     559\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"maiores 23\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652adc3d-bb83-4f99-99c6-cb8bc8956f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cd_instituic</th>\n",
       "      <th>cd_curso</th>\n",
       "      <th>sexo</th>\n",
       "      <th>ord_ingresso</th>\n",
       "      <th>cd_tip_est_sec</th>\n",
       "      <th>cd_hab_ant</th>\n",
       "      <th>cd_regime</th>\n",
       "      <th>idadeReal</th>\n",
       "      <th>trabalhadorEstudante</th>\n",
       "      <th>...</th>\n",
       "      <th>internacional</th>\n",
       "      <th>maiores 23</th>\n",
       "      <th>outros</th>\n",
       "      <th>1geracao</th>\n",
       "      <th>nota10-12</th>\n",
       "      <th>nota12-14</th>\n",
       "      <th>nota14-16</th>\n",
       "      <th>nota16-18</th>\n",
       "      <th>nota18-20</th>\n",
       "      <th>Abandono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "      <td>5411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4930.722602</td>\n",
       "      <td>0.816855</td>\n",
       "      <td>12.098318</td>\n",
       "      <td>0.497690</td>\n",
       "      <td>1.868601</td>\n",
       "      <td>1.059139</td>\n",
       "      <td>4.080022</td>\n",
       "      <td>0.662539</td>\n",
       "      <td>21.820736</td>\n",
       "      <td>0.313805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.103308</td>\n",
       "      <td>0.168176</td>\n",
       "      <td>0.802994</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>0.420625</td>\n",
       "      <td>0.327111</td>\n",
       "      <td>0.104047</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.118278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2435.365596</td>\n",
       "      <td>1.025687</td>\n",
       "      <td>6.968396</td>\n",
       "      <td>0.500041</td>\n",
       "      <td>1.280710</td>\n",
       "      <td>0.332213</td>\n",
       "      <td>8.854611</td>\n",
       "      <td>0.921884</td>\n",
       "      <td>7.075904</td>\n",
       "      <td>0.464081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224502</td>\n",
       "      <td>0.304389</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>0.397774</td>\n",
       "      <td>0.346339</td>\n",
       "      <td>0.493705</td>\n",
       "      <td>0.469202</td>\n",
       "      <td>0.305350</td>\n",
       "      <td>0.092802</td>\n",
       "      <td>0.322966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>227.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2833.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4862.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7142.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9347.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  cd_instituic     cd_curso         sexo  ord_ingresso  \\\n",
       "count  5411.000000   5411.000000  5411.000000  5411.000000   5411.000000   \n",
       "mean   4930.722602      0.816855    12.098318     0.497690      1.868601   \n",
       "std    2435.365596      1.025687     6.968396     0.500041      1.280710   \n",
       "min     227.000000      0.000000     0.000000     0.000000      1.000000   \n",
       "25%    2833.500000      0.000000     6.000000     0.000000      1.000000   \n",
       "50%    4862.000000      0.000000    12.000000     0.000000      1.000000   \n",
       "75%    7142.500000      1.000000    18.000000     1.000000      2.000000   \n",
       "max    9347.000000      3.000000    24.000000     1.000000      6.000000   \n",
       "\n",
       "       cd_tip_est_sec   cd_hab_ant    cd_regime    idadeReal  \\\n",
       "count     5411.000000  5411.000000  5411.000000  5411.000000   \n",
       "mean         1.059139     4.080022     0.662539    21.820736   \n",
       "std          0.332213     8.854611     0.921884     7.075904   \n",
       "min          0.000000     0.000000     0.000000     1.000000   \n",
       "25%          1.000000     1.000000     0.000000    18.000000   \n",
       "50%          1.000000     1.000000     0.000000    19.000000   \n",
       "75%          1.000000     1.000000     2.000000    22.000000   \n",
       "max          3.000000    31.000000     2.000000    69.000000   \n",
       "\n",
       "       trabalhadorEstudante  ...  internacional   maiores 23       outros  \\\n",
       "count           5411.000000  ...    5411.000000  5411.000000  5411.000000   \n",
       "mean               0.313805  ...       0.053225     0.103308     0.168176   \n",
       "std                0.464081  ...       0.224502     0.304389     0.374057   \n",
       "min                0.000000  ...       0.000000     0.000000     0.000000   \n",
       "25%                0.000000  ...       0.000000     0.000000     0.000000   \n",
       "50%                0.000000  ...       0.000000     0.000000     0.000000   \n",
       "75%                1.000000  ...       0.000000     0.000000     0.000000   \n",
       "max                1.000000  ...       1.000000     1.000000     1.000000   \n",
       "\n",
       "          1geracao    nota10-12    nota12-14    nota14-16    nota16-18  \\\n",
       "count  5411.000000  5411.000000  5411.000000  5411.000000  5411.000000   \n",
       "mean      0.802994     0.139346     0.420625     0.327111     0.104047   \n",
       "std       0.397774     0.346339     0.493705     0.469202     0.305350   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     1.000000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         nota18-20     Abandono  \n",
       "count  5411.000000  5411.000000  \n",
       "mean      0.008686     0.118278  \n",
       "std       0.092802     0.322966  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.000000     0.000000  \n",
       "75%       0.000000     0.000000  \n",
       "max       1.000000     1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd70d87-e847-4822-acc6-39a152826fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5411 entries, 0 to 5410\n",
      "Data columns (total 34 columns):\n",
      " #   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      " 0   Unnamed: 0             5411 non-null   int64\n",
      " 1   cd_instituic           5411 non-null   int64\n",
      " 2   cd_curso               5411 non-null   int64\n",
      " 3   sexo                   5411 non-null   int64\n",
      " 4   ord_ingresso           5411 non-null   int64\n",
      " 5   cd_tip_est_sec         5411 non-null   int64\n",
      " 6   cd_hab_ant             5411 non-null   int64\n",
      " 7   cd_regime              5411 non-null   int64\n",
      " 8   idadeReal              5411 non-null   int64\n",
      " 9   trabalhadorEstudante   5411 non-null   int64\n",
      " 10  cd_inst_hab_ant        5411 non-null   int64\n",
      " 11  estado_civil           5411 non-null   int64\n",
      " 12  cd_cur_hab_ant         5411 non-null   int64\n",
      " 13  Ensino Basico Mae      5411 non-null   int64\n",
      " 14  Ensino Basico Pai      5411 non-null   int64\n",
      " 15  Ensino SecundÃ¡rio Mae  5411 non-null   int64\n",
      " 16  Ensino SecundÃ¡rio Pai  5411 non-null   int64\n",
      " 17  Ensino Superior Mae    5411 non-null   int64\n",
      " 18  Ensino Superior Pai    5411 non-null   int64\n",
      " 19  Ensino Outros Mae      5411 non-null   int64\n",
      " 20  Ensino Outros Pai      5411 non-null   int64\n",
      " 21  pais trabalham         5411 non-null   int64\n",
      " 22  Portugues              5411 non-null   int64\n",
      " 23  CNA                    5411 non-null   int64\n",
      " 24  internacional          5411 non-null   int64\n",
      " 25  maiores 23             5411 non-null   int64\n",
      " 26  outros                 5411 non-null   int64\n",
      " 27  1geracao               5411 non-null   int64\n",
      " 28  nota10-12              5411 non-null   int64\n",
      " 29  nota12-14              5411 non-null   int64\n",
      " 30  nota14-16              5411 non-null   int64\n",
      " 31  nota16-18              5411 non-null   int64\n",
      " 32  nota18-20              5411 non-null   int64\n",
      " 33  Abandono               5411 non-null   int64\n",
      "dtypes: int64(34)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a56f4e3-9239-4d93-a812-d44fef43cd86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               0\n",
       "cd_instituic             0\n",
       "cd_curso                 0\n",
       "sexo                     0\n",
       "ord_ingresso             0\n",
       "cd_tip_est_sec           0\n",
       "cd_hab_ant               0\n",
       "cd_regime                0\n",
       "idadeReal                0\n",
       "trabalhadorEstudante     0\n",
       "cd_inst_hab_ant          0\n",
       "estado_civil             0\n",
       "cd_cur_hab_ant           0\n",
       "Ensino Basico Mae        0\n",
       "Ensino Basico Pai        0\n",
       "Ensino SecundÃ¡rio Mae    0\n",
       "Ensino SecundÃ¡rio Pai    0\n",
       "Ensino Superior Mae      0\n",
       "Ensino Superior Pai      0\n",
       "Ensino Outros Mae        0\n",
       "Ensino Outros Pai        0\n",
       "pais trabalham           0\n",
       "Portugues                0\n",
       "CNA                      0\n",
       "internacional            0\n",
       "maiores 23               0\n",
       "outros                   0\n",
       "1geracao                 0\n",
       "nota10-12                0\n",
       "nota12-14                0\n",
       "nota14-16                0\n",
       "nota16-18                0\n",
       "nota18-20                0\n",
       "Abandono                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6e4179-664a-40c3-b122-1f06a260ff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (4328, 32), (1083, 32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum().sum()\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = data.drop(columns=['Abandono'])\n",
    "y = data['Abandono']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_normalized, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32))\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val.astype(np.float32))\n",
    "y_val_tensor = torch.tensor(y_val.values.astype(np.float32))\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "missing_values, X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3224d4-16ba-490c-8305-9940cdb4f164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2856523096561432, Validation Loss: 0.36089260350255403\n",
      "Epoch 2, Training Loss: 0.15277230739593506, Validation Loss: 0.32701579349882465\n",
      "Epoch 3, Training Loss: 0.31767430901527405, Validation Loss: 0.3257924274486654\n",
      "Epoch 4, Training Loss: 0.31403690576553345, Validation Loss: 0.33243824804530425\n",
      "Epoch 5, Training Loss: 0.2910573184490204, Validation Loss: 0.34173959931906533\n",
      "Epoch 6, Training Loss: 0.2009073942899704, Validation Loss: 0.3378655945553499\n",
      "Epoch 7, Training Loss: 0.3522256910800934, Validation Loss: 0.34167397022247314\n",
      "Epoch 8, Training Loss: 0.389475554227829, Validation Loss: 0.3372388035058975\n",
      "Epoch 9, Training Loss: 0.3955230712890625, Validation Loss: 0.3366493346060024\n",
      "Epoch 10, Training Loss: 0.12826064229011536, Validation Loss: 0.3351149567786385\n",
      "Epoch 11, Training Loss: 0.25078898668289185, Validation Loss: 0.3326301636064754\n",
      "Epoch 12, Training Loss: 0.25014275312423706, Validation Loss: 0.332107597414185\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, nn_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, nn_size)  # Increased size\n",
    "        self.bn1 = nn.BatchNorm1d(nn_size)  # Batch normalization\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(nn_size, nn_size)\n",
    "        self.bn2 = nn.BatchNorm1d(nn_size)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(nn_size, nn_size)\n",
    "        self.bn3 = nn.BatchNorm1d(nn_size)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(nn_size, 1)\n",
    "\n",
    "        # Weight Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = Net(input_size, 2048).to(device)\n",
    "criterion = nn.BCELoss()  # Consider weighted loss if class imbalance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate decay\n",
    "\n",
    "# Early Stopping Parameters\n",
    "early_stopping_patience = 10\n",
    "min_val_loss = np.inf\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100  # Set your number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels.view(-1, 1)).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "        if no_improvement_epochs >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d2733b-ef6a-495a-9b47-75b574a52f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bda4a3-c3bc-4eca-9b06-334332e093af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n",
      "Epoch 1/100 \tTraining Loss: 0.113475 \tValidation Loss: 0.074512\n",
      "Training Precision: 0.12, Recall: 1.00, F1 Score: 0.21\n",
      "Validation Precision: 0.12, Recall: 1.00, F1 Score: 0.22\n",
      "Epoch 2/100 \tTraining Loss: 0.060121 \tValidation Loss: 0.037300\n",
      "Training Precision: 0.12, Recall: 1.00, F1 Score: 0.21\n",
      "Validation Precision: 0.12, Recall: 1.00, F1 Score: 0.22\n",
      "Epoch 3/100 \tTraining Loss: 0.039896 \tValidation Loss: 0.027262\n",
      "Training Precision: 0.12, Recall: 1.00, F1 Score: 0.21\n",
      "Validation Precision: 0.13, Recall: 1.00, F1 Score: 0.24\n",
      "Epoch 4/100 \tTraining Loss: 0.035152 \tValidation Loss: 0.025173\n",
      "Training Precision: 0.14, Recall: 1.00, F1 Score: 0.24\n",
      "Validation Precision: 0.16, Recall: 1.00, F1 Score: 0.27\n",
      "Epoch 5/100 \tTraining Loss: 0.031946 \tValidation Loss: 0.027405\n",
      "Training Precision: 0.15, Recall: 1.00, F1 Score: 0.26\n",
      "Validation Precision: 0.17, Recall: 1.00, F1 Score: 0.29\n",
      "Epoch 6/100 \tTraining Loss: 0.027197 \tValidation Loss: 0.023671\n",
      "Training Precision: 0.16, Recall: 1.00, F1 Score: 0.27\n",
      "Validation Precision: 0.19, Recall: 1.00, F1 Score: 0.32\n",
      "Epoch 7/100 \tTraining Loss: 0.026147 \tValidation Loss: 0.024912\n",
      "Training Precision: 0.17, Recall: 1.00, F1 Score: 0.30\n",
      "Validation Precision: 0.20, Recall: 1.00, F1 Score: 0.34\n",
      "Epoch 8/100 \tTraining Loss: 0.026711 \tValidation Loss: 0.020536\n",
      "Training Precision: 0.18, Recall: 1.00, F1 Score: 0.30\n",
      "Validation Precision: 0.20, Recall: 1.00, F1 Score: 0.33\n",
      "Epoch 9/100 \tTraining Loss: 0.021688 \tValidation Loss: 0.018343\n",
      "Training Precision: 0.19, Recall: 1.00, F1 Score: 0.32\n",
      "Validation Precision: 0.21, Recall: 1.00, F1 Score: 0.35\n",
      "Epoch 10/100 \tTraining Loss: 0.021696 \tValidation Loss: 0.020442\n",
      "Training Precision: 0.21, Recall: 1.00, F1 Score: 0.34\n",
      "Validation Precision: 0.23, Recall: 1.00, F1 Score: 0.38\n",
      "Epoch 11/100 \tTraining Loss: 0.018072 \tValidation Loss: 0.016117\n",
      "Training Precision: 0.21, Recall: 1.00, F1 Score: 0.35\n",
      "Validation Precision: 0.24, Recall: 1.00, F1 Score: 0.39\n",
      "Epoch 12/100 \tTraining Loss: 0.018382 \tValidation Loss: 0.017869\n",
      "Training Precision: 0.23, Recall: 1.00, F1 Score: 0.37\n",
      "Validation Precision: 0.24, Recall: 1.00, F1 Score: 0.39\n",
      "Epoch 13/100 \tTraining Loss: 0.017896 \tValidation Loss: 0.016372\n",
      "Training Precision: 0.24, Recall: 1.00, F1 Score: 0.39\n",
      "Validation Precision: 0.27, Recall: 1.00, F1 Score: 0.43\n",
      "Epoch 14/100 \tTraining Loss: 0.017665 \tValidation Loss: 0.017544\n",
      "Training Precision: 0.24, Recall: 1.00, F1 Score: 0.39\n",
      "Validation Precision: 0.29, Recall: 1.00, F1 Score: 0.45\n",
      "Epoch 15/100 \tTraining Loss: 0.017525 \tValidation Loss: 0.016120\n",
      "Training Precision: 0.25, Recall: 1.00, F1 Score: 0.41\n",
      "Validation Precision: 0.28, Recall: 1.00, F1 Score: 0.44\n",
      "Epoch 16/100 \tTraining Loss: 0.014150 \tValidation Loss: 0.016921\n",
      "Training Precision: 0.26, Recall: 1.00, F1 Score: 0.41\n",
      "Validation Precision: 0.32, Recall: 1.00, F1 Score: 0.48\n",
      "Epoch 17/100 \tTraining Loss: 0.014130 \tValidation Loss: 0.015961\n",
      "Training Precision: 0.28, Recall: 1.00, F1 Score: 0.44\n",
      "Validation Precision: 0.32, Recall: 1.00, F1 Score: 0.49\n",
      "Epoch 18/100 \tTraining Loss: 0.015473 \tValidation Loss: 0.020264\n",
      "Training Precision: 0.28, Recall: 1.00, F1 Score: 0.44\n",
      "Validation Precision: 0.32, Recall: 1.00, F1 Score: 0.48\n",
      "Epoch 19/100 \tTraining Loss: 0.013698 \tValidation Loss: 0.019404\n",
      "Training Precision: 0.28, Recall: 1.00, F1 Score: 0.44\n",
      "Validation Precision: 0.32, Recall: 1.00, F1 Score: 0.49\n",
      "Epoch 20/100 \tTraining Loss: 0.014869 \tValidation Loss: 0.018435\n",
      "Training Precision: 0.29, Recall: 1.00, F1 Score: 0.45\n",
      "Validation Precision: 0.32, Recall: 1.00, F1 Score: 0.49\n",
      "Epoch 21/100 \tTraining Loss: 0.012938 \tValidation Loss: 0.016787\n",
      "Training Precision: 0.29, Recall: 1.00, F1 Score: 0.45\n",
      "Validation Precision: 0.33, Recall: 1.00, F1 Score: 0.50\n",
      "Epoch 22/100 \tTraining Loss: 0.012149 \tValidation Loss: 0.019282\n",
      "Training Precision: 0.30, Recall: 1.00, F1 Score: 0.46\n",
      "Validation Precision: 0.34, Recall: 1.00, F1 Score: 0.51\n",
      "Epoch 23/100 \tTraining Loss: 0.012656 \tValidation Loss: 0.020719\n",
      "Training Precision: 0.31, Recall: 1.00, F1 Score: 0.47\n",
      "Validation Precision: 0.35, Recall: 1.00, F1 Score: 0.52\n",
      "Epoch 24/100 \tTraining Loss: 0.013757 \tValidation Loss: 0.017498\n",
      "Training Precision: 0.31, Recall: 1.00, F1 Score: 0.47\n",
      "Validation Precision: 0.33, Recall: 1.00, F1 Score: 0.49\n",
      "Epoch 25/100 \tTraining Loss: 0.011815 \tValidation Loss: 0.018071\n",
      "Training Precision: 0.30, Recall: 1.00, F1 Score: 0.46\n",
      "Validation Precision: 0.34, Recall: 1.00, F1 Score: 0.51\n",
      "Epoch 26/100 \tTraining Loss: 0.010843 \tValidation Loss: 0.016339\n",
      "Training Precision: 0.32, Recall: 1.00, F1 Score: 0.48\n",
      "Validation Precision: 0.36, Recall: 1.00, F1 Score: 0.53\n",
      "Epoch 27/100 \tTraining Loss: 0.010551 \tValidation Loss: 0.018129\n",
      "Training Precision: 0.33, Recall: 0.99, F1 Score: 0.50\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 28/100 \tTraining Loss: 0.011053 \tValidation Loss: 0.019183\n",
      "Training Precision: 0.33, Recall: 1.00, F1 Score: 0.50\n",
      "Validation Precision: 0.36, Recall: 1.00, F1 Score: 0.53\n",
      "Epoch 29/100 \tTraining Loss: 0.009236 \tValidation Loss: 0.017922\n",
      "Training Precision: 0.33, Recall: 1.00, F1 Score: 0.50\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 30/100 \tTraining Loss: 0.010388 \tValidation Loss: 0.019411\n",
      "Training Precision: 0.34, Recall: 1.00, F1 Score: 0.50\n",
      "Validation Precision: 0.38, Recall: 1.00, F1 Score: 0.55\n",
      "Epoch 31/100 \tTraining Loss: 0.011407 \tValidation Loss: 0.016733\n",
      "Training Precision: 0.33, Recall: 0.99, F1 Score: 0.50\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 32/100 \tTraining Loss: 0.008770 \tValidation Loss: 0.019851\n",
      "Training Precision: 0.34, Recall: 0.99, F1 Score: 0.50\n",
      "Validation Precision: 0.38, Recall: 1.00, F1 Score: 0.55\n",
      "Epoch 33/100 \tTraining Loss: 0.009786 \tValidation Loss: 0.018707\n",
      "Training Precision: 0.34, Recall: 0.99, F1 Score: 0.50\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 34/100 \tTraining Loss: 0.008325 \tValidation Loss: 0.017710\n",
      "Training Precision: 0.34, Recall: 0.99, F1 Score: 0.51\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 35/100 \tTraining Loss: 0.010932 \tValidation Loss: 0.021477\n",
      "Training Precision: 0.34, Recall: 0.99, F1 Score: 0.51\n",
      "Validation Precision: 0.39, Recall: 1.00, F1 Score: 0.56\n",
      "Epoch 36/100 \tTraining Loss: 0.009667 \tValidation Loss: 0.016901\n",
      "Training Precision: 0.35, Recall: 0.99, F1 Score: 0.51\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 37/100 \tTraining Loss: 0.008909 \tValidation Loss: 0.018570\n",
      "Training Precision: 0.35, Recall: 0.99, F1 Score: 0.51\n",
      "Validation Precision: 0.39, Recall: 1.00, F1 Score: 0.57\n",
      "Epoch 38/100 \tTraining Loss: 0.008407 \tValidation Loss: 0.015154\n",
      "Training Precision: 0.35, Recall: 0.99, F1 Score: 0.52\n",
      "Validation Precision: 0.39, Recall: 1.00, F1 Score: 0.56\n",
      "Epoch 39/100 \tTraining Loss: 0.008358 \tValidation Loss: 0.019150\n",
      "Training Precision: 0.37, Recall: 0.99, F1 Score: 0.54\n",
      "Validation Precision: 0.40, Recall: 1.00, F1 Score: 0.57\n",
      "Epoch 40/100 \tTraining Loss: 0.009286 \tValidation Loss: 0.019741\n",
      "Training Precision: 0.35, Recall: 0.99, F1 Score: 0.52\n",
      "Validation Precision: 0.37, Recall: 1.00, F1 Score: 0.54\n",
      "Epoch 41/100 \tTraining Loss: 0.009701 \tValidation Loss: 0.019744\n",
      "Training Precision: 0.36, Recall: 0.99, F1 Score: 0.52\n",
      "Validation Precision: 0.39, Recall: 1.00, F1 Score: 0.56\n",
      "Epoch 42/100 \tTraining Loss: 0.009092 \tValidation Loss: 0.017241\n",
      "Training Precision: 0.35, Recall: 0.99, F1 Score: 0.52\n",
      "Validation Precision: 0.38, Recall: 1.00, F1 Score: 0.55\n",
      "Epoch 43/100 \tTraining Loss: 0.007467 \tValidation Loss: 0.018034\n",
      "Training Precision: 0.36, Recall: 0.99, F1 Score: 0.53\n",
      "Validation Precision: 0.38, Recall: 1.00, F1 Score: 0.55\n",
      "Epoch 44/100 \tTraining Loss: 0.006503 \tValidation Loss: 0.019168\n",
      "Training Precision: 0.36, Recall: 0.99, F1 Score: 0.53\n",
      "Validation Precision: 0.40, Recall: 1.00, F1 Score: 0.57\n",
      "Epoch 45/100 \tTraining Loss: 0.007190 \tValidation Loss: 0.020156\n",
      "Training Precision: 0.37, Recall: 0.99, F1 Score: 0.53\n",
      "Validation Precision: 0.40, Recall: 1.00, F1 Score: 0.57\n",
      "Epoch 46/100 \tTraining Loss: 0.006951 \tValidation Loss: 0.020910\n",
      "Training Precision: 0.37, Recall: 0.99, F1 Score: 0.54\n",
      "Validation Precision: 0.39, Recall: 1.00, F1 Score: 0.57\n",
      "Epoch 47/100 \tTraining Loss: 0.006463 \tValidation Loss: 0.019164\n",
      "Training Precision: 0.38, Recall: 0.99, F1 Score: 0.54\n",
      "Validation Precision: 0.41, Recall: 1.00, F1 Score: 0.59\n",
      "Epoch 48/100 \tTraining Loss: 0.007167 \tValidation Loss: 0.018691\n",
      "Training Precision: 0.38, Recall: 0.99, F1 Score: 0.55\n",
      "Validation Precision: 0.41, Recall: 1.00, F1 Score: 0.59\n",
      "Epoch 49/100 \tTraining Loss: 0.006841 \tValidation Loss: 0.021520\n",
      "Training Precision: 0.39, Recall: 0.99, F1 Score: 0.56\n",
      "Validation Precision: 0.43, Recall: 1.00, F1 Score: 0.60\n",
      "Epoch 50/100 \tTraining Loss: 0.006439 \tValidation Loss: 0.020435\n",
      "Training Precision: 0.39, Recall: 0.99, F1 Score: 0.56\n",
      "Validation Precision: 0.42, Recall: 1.00, F1 Score: 0.59\n",
      "Epoch 51/100 \tTraining Loss: 0.005826 \tValidation Loss: 0.019659\n",
      "Training Precision: 0.39, Recall: 0.99, F1 Score: 0.55\n",
      "Validation Precision: 0.40, Recall: 1.00, F1 Score: 0.58\n",
      "Epoch 52/100 \tTraining Loss: 0.006088 \tValidation Loss: 0.023798\n",
      "Training Precision: 0.39, Recall: 0.99, F1 Score: 0.56\n",
      "Validation Precision: 0.44, Recall: 1.00, F1 Score: 0.61\n",
      "Epoch 53/100 \tTraining Loss: 0.006297 \tValidation Loss: 0.018547\n",
      "Training Precision: 0.38, Recall: 0.99, F1 Score: 0.55\n",
      "Validation Precision: 0.43, Recall: 1.00, F1 Score: 0.60\n",
      "Epoch 54/100 \tTraining Loss: 0.007222 \tValidation Loss: 0.019244\n",
      "Training Precision: 0.38, Recall: 0.99, F1 Score: 0.55\n",
      "Validation Precision: 0.43, Recall: 1.00, F1 Score: 0.60\n",
      "Epoch 55/100 \tTraining Loss: 0.005732 \tValidation Loss: 0.018659\n",
      "Training Precision: 0.39, Recall: 0.99, F1 Score: 0.56\n",
      "Validation Precision: 0.44, Recall: 1.00, F1 Score: 0.61\n",
      "Epoch 56/100 \tTraining Loss: 0.006345 \tValidation Loss: 0.019579\n",
      "Training Precision: 0.40, Recall: 0.99, F1 Score: 0.57\n",
      "Validation Precision: 0.43, Recall: 1.00, F1 Score: 0.60\n",
      "Epoch 57/100 \tTraining Loss: 0.004529 \tValidation Loss: 0.022426\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 58/100 \tTraining Loss: 0.007185 \tValidation Loss: 0.023017\n",
      "Training Precision: 0.42, Recall: 0.99, F1 Score: 0.59\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 59/100 \tTraining Loss: 0.006943 \tValidation Loss: 0.023145\n",
      "Training Precision: 0.40, Recall: 0.99, F1 Score: 0.57\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 60/100 \tTraining Loss: 0.006970 \tValidation Loss: 0.022281\n",
      "Training Precision: 0.40, Recall: 0.99, F1 Score: 0.57\n",
      "Validation Precision: 0.46, Recall: 1.00, F1 Score: 0.63\n",
      "Epoch 61/100 \tTraining Loss: 0.006348 \tValidation Loss: 0.021136\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.46, Recall: 1.00, F1 Score: 0.63\n",
      "Epoch 62/100 \tTraining Loss: 0.006543 \tValidation Loss: 0.018506\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 63/100 \tTraining Loss: 0.005308 \tValidation Loss: 0.020121\n",
      "Training Precision: 0.40, Recall: 0.99, F1 Score: 0.57\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 64/100 \tTraining Loss: 0.005335 \tValidation Loss: 0.016978\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.43, Recall: 1.00, F1 Score: 0.60\n",
      "Epoch 65/100 \tTraining Loss: 0.006090 \tValidation Loss: 0.017493\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.44, Recall: 1.00, F1 Score: 0.61\n",
      "Epoch 66/100 \tTraining Loss: 0.005014 \tValidation Loss: 0.019922\n",
      "Training Precision: 0.41, Recall: 0.99, F1 Score: 0.58\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 67/100 \tTraining Loss: 0.005301 \tValidation Loss: 0.020360\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.61\n",
      "Validation Precision: 0.51, Recall: 1.00, F1 Score: 0.67\n",
      "Epoch 68/100 \tTraining Loss: 0.005074 \tValidation Loss: 0.022587\n",
      "Training Precision: 0.43, Recall: 0.99, F1 Score: 0.60\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 69/100 \tTraining Loss: 0.005798 \tValidation Loss: 0.020537\n",
      "Training Precision: 0.43, Recall: 0.99, F1 Score: 0.60\n",
      "Validation Precision: 0.48, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 70/100 \tTraining Loss: 0.004944 \tValidation Loss: 0.019894\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.61\n",
      "Validation Precision: 0.48, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 71/100 \tTraining Loss: 0.004491 \tValidation Loss: 0.020002\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.53, Recall: 1.00, F1 Score: 0.70\n",
      "Epoch 72/100 \tTraining Loss: 0.005573 \tValidation Loss: 0.019545\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.48, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 73/100 \tTraining Loss: 0.006153 \tValidation Loss: 0.019876\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.61\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 74/100 \tTraining Loss: 0.004768 \tValidation Loss: 0.018748\n",
      "Training Precision: 0.43, Recall: 0.99, F1 Score: 0.60\n",
      "Validation Precision: 0.45, Recall: 1.00, F1 Score: 0.62\n",
      "Epoch 75/100 \tTraining Loss: 0.004278 \tValidation Loss: 0.019025\n",
      "Training Precision: 0.43, Recall: 0.99, F1 Score: 0.60\n",
      "Validation Precision: 0.48, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 76/100 \tTraining Loss: 0.004216 \tValidation Loss: 0.022626\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.56, Recall: 1.00, F1 Score: 0.72\n",
      "Epoch 77/100 \tTraining Loss: 0.004966 \tValidation Loss: 0.022504\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.52, Recall: 1.00, F1 Score: 0.68\n",
      "Epoch 78/100 \tTraining Loss: 0.006863 \tValidation Loss: 0.020792\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 79/100 \tTraining Loss: 0.006313 \tValidation Loss: 0.019755\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.47, Recall: 1.00, F1 Score: 0.64\n",
      "Epoch 80/100 \tTraining Loss: 0.005686 \tValidation Loss: 0.022332\n",
      "Training Precision: 0.45, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.50, Recall: 1.00, F1 Score: 0.67\n",
      "Epoch 81/100 \tTraining Loss: 0.004902 \tValidation Loss: 0.020805\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.60\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 82/100 \tTraining Loss: 0.004711 \tValidation Loss: 0.022382\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.61\n",
      "Validation Precision: 0.52, Recall: 1.00, F1 Score: 0.68\n",
      "Epoch 83/100 \tTraining Loss: 0.004977 \tValidation Loss: 0.019914\n",
      "Training Precision: 0.45, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 84/100 \tTraining Loss: 0.003936 \tValidation Loss: 0.020658\n",
      "Training Precision: 0.45, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.50, Recall: 1.00, F1 Score: 0.67\n",
      "Epoch 85/100 \tTraining Loss: 0.005343 \tValidation Loss: 0.021633\n",
      "Training Precision: 0.44, Recall: 0.99, F1 Score: 0.61\n",
      "Validation Precision: 0.53, Recall: 1.00, F1 Score: 0.69\n",
      "Epoch 86/100 \tTraining Loss: 0.004655 \tValidation Loss: 0.020690\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.50, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 87/100 \tTraining Loss: 0.004469 \tValidation Loss: 0.021551\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.64\n",
      "Validation Precision: 0.51, Recall: 1.00, F1 Score: 0.68\n",
      "Epoch 88/100 \tTraining Loss: 0.004799 \tValidation Loss: 0.022300\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.51, Recall: 1.00, F1 Score: 0.68\n",
      "Epoch 89/100 \tTraining Loss: 0.004441 \tValidation Loss: 0.018907\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.48, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 90/100 \tTraining Loss: 0.004333 \tValidation Loss: 0.019903\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.64\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 91/100 \tTraining Loss: 0.005070 \tValidation Loss: 0.020311\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.64\n",
      "Validation Precision: 0.50, Recall: 1.00, F1 Score: 0.67\n",
      "Epoch 92/100 \tTraining Loss: 0.004565 \tValidation Loss: 0.021892\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.54, Recall: 1.00, F1 Score: 0.71\n",
      "Epoch 93/100 \tTraining Loss: 0.003772 \tValidation Loss: 0.019831\n",
      "Training Precision: 0.45, Recall: 0.99, F1 Score: 0.62\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.65\n",
      "Epoch 94/100 \tTraining Loss: 0.004881 \tValidation Loss: 0.020705\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.64\n",
      "Validation Precision: 0.51, Recall: 1.00, F1 Score: 0.68\n",
      "Epoch 95/100 \tTraining Loss: 0.006370 \tValidation Loss: 0.020221\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 96/100 \tTraining Loss: 0.003713 \tValidation Loss: 0.020669\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.53, Recall: 1.00, F1 Score: 0.70\n",
      "Epoch 97/100 \tTraining Loss: 0.004888 \tValidation Loss: 0.022718\n",
      "Training Precision: 0.46, Recall: 0.99, F1 Score: 0.63\n",
      "Validation Precision: 0.52, Recall: 1.00, F1 Score: 0.69\n",
      "Epoch 98/100 \tTraining Loss: 0.004398 \tValidation Loss: 0.019732\n",
      "Training Precision: 0.48, Recall: 0.99, F1 Score: 0.65\n",
      "Validation Precision: 0.53, Recall: 1.00, F1 Score: 0.69\n",
      "Epoch 99/100 \tTraining Loss: 0.004115 \tValidation Loss: 0.020715\n",
      "Training Precision: 0.47, Recall: 0.99, F1 Score: 0.64\n",
      "Validation Precision: 0.49, Recall: 1.00, F1 Score: 0.66\n",
      "Epoch 100/100 \tTraining Loss: 0.005144 \tValidation Loss: 0.019892\n",
      "Training Precision: 0.49, Recall: 0.99, F1 Score: 0.65\n",
      "Validation Precision: 0.56, Recall: 1.00, F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# data.drop('Unnamed: 0', axis=1, inplace=True)  # Remove irrelevant column\n",
    "\n",
    "# Encoding categorical data (example for 'sexo' column)\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# encoded_sex = encoder.fit_transform(data[['sexo']])\n",
    "# data = data.drop('sexo', axis=1)\n",
    "# data = data.join(pd.DataFrame(encoded_sex, columns=encoder.get_feature_names(['sexo'])))\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('Abandono', axis=1))\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, data['Abandono'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EnhancedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128) # Increased neurons\n",
    "        self.dropout1 = nn.Dropout(0.3)        # Dropout layer\n",
    "        self.fc2 = nn.Linear(128, 128)       # New layer\n",
    "        self.dropout2 = nn.Dropout(0.2)        # Dropout layer\n",
    "        self.fc3 = nn.Linear(128, 128)          # Reduced neurons\n",
    "        self.fc4 = nn.Linear(128, 1) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Sigmoid activation for binary classification\n",
    "        return x\n",
    "\n",
    "    \n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "\n",
    "# Assuming your feature count is 'n_features'\n",
    "n_features = X_train.shape[1]  # Replace with actual number of features\n",
    "model = EnhancedNet(input_size=n_features).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()  # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "\n",
    "epochs = 100  # Number of epochs\n",
    "\n",
    "# Prepare validation set\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_train_labels = []\n",
    "    all_train_preds = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1)  # Adjust labels to match output dimensions\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Save predictions and labels for metrics calculation\n",
    "        preds = torch.sigmoid(outputs).round().detach().cpu().numpy()  # Corrected line\n",
    "        all_train_preds.extend(preds)\n",
    "        all_train_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1)  # Adjust labels to match output dimensions\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Save predictions and labels for metrics calculation\n",
    "            preds = torch.sigmoid(outputs).round().detach().cpu().numpy()  # Corrected line\n",
    "            all_val_preds.extend(preds)\n",
    "            all_val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # Calculate and print training metrics\n",
    "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_train_labels, all_train_preds, average='binary')\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {val_loss:.6f}')\n",
    "    print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1 Score: {train_f1:.2f}')\n",
    "    print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4110fd-cd76-4fbb-ac8d-e618ebed2823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 \tTraining Loss: 0.136196 \tValidation Loss: 0.058979\n",
      "Training Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n",
      "Validation Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n",
      "Epoch 2/100 \tTraining Loss: 0.054202 \tValidation Loss: 0.046681\n",
      "Training Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n",
      "Validation Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/db/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 \tTraining Loss: 0.043999 \tValidation Loss: 0.037658\n",
      "Training Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n",
      "Validation Precision: 0.00, Recall: 0.00, F1 Score: 0.00\n",
      "Epoch 4/100 \tTraining Loss: 0.038126 \tValidation Loss: 0.033731\n",
      "Training Precision: 1.00, Recall: 0.01, F1 Score: 0.02\n",
      "Validation Precision: 1.00, Recall: 0.08, F1 Score: 0.15\n",
      "Epoch 5/100 \tTraining Loss: 0.032938 \tValidation Loss: 0.029877\n",
      "Training Precision: 0.98, Recall: 0.22, F1 Score: 0.36\n",
      "Validation Precision: 1.00, Recall: 0.31, F1 Score: 0.47\n",
      "Epoch 6/100 \tTraining Loss: 0.028934 \tValidation Loss: 0.024070\n",
      "Training Precision: 0.97, Recall: 0.41, F1 Score: 0.58\n",
      "Validation Precision: 1.00, Recall: 0.44, F1 Score: 0.61\n",
      "Epoch 7/100 \tTraining Loss: 0.025821 \tValidation Loss: 0.021746\n",
      "Training Precision: 0.98, Recall: 0.52, F1 Score: 0.68\n",
      "Validation Precision: 1.00, Recall: 0.51, F1 Score: 0.67\n",
      "Epoch 8/100 \tTraining Loss: 0.024817 \tValidation Loss: 0.024160\n",
      "Training Precision: 0.97, Recall: 0.55, F1 Score: 0.70\n",
      "Validation Precision: 1.00, Recall: 0.49, F1 Score: 0.66\n",
      "Epoch 9/100 \tTraining Loss: 0.023396 \tValidation Loss: 0.020649\n",
      "Training Precision: 0.96, Recall: 0.60, F1 Score: 0.74\n",
      "Validation Precision: 0.97, Recall: 0.65, F1 Score: 0.78\n",
      "Epoch 10/100 \tTraining Loss: 0.023067 \tValidation Loss: 0.018899\n",
      "Training Precision: 0.96, Recall: 0.58, F1 Score: 0.72\n",
      "Validation Precision: 0.99, Recall: 0.68, F1 Score: 0.81\n",
      "Epoch 11/100 \tTraining Loss: 0.022294 \tValidation Loss: 0.020785\n",
      "Training Precision: 0.97, Recall: 0.63, F1 Score: 0.76\n",
      "Validation Precision: 1.00, Recall: 0.57, F1 Score: 0.72\n",
      "Epoch 12/100 \tTraining Loss: 0.019233 \tValidation Loss: 0.018477\n",
      "Training Precision: 0.98, Recall: 0.71, F1 Score: 0.82\n",
      "Validation Precision: 0.99, Recall: 0.66, F1 Score: 0.79\n",
      "Epoch 13/100 \tTraining Loss: 0.018278 \tValidation Loss: 0.021481\n",
      "Training Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Validation Precision: 0.99, Recall: 0.60, F1 Score: 0.74\n",
      "Epoch 14/100 \tTraining Loss: 0.018643 \tValidation Loss: 0.016691\n",
      "Training Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Validation Precision: 1.00, Recall: 0.69, F1 Score: 0.82\n",
      "Epoch 15/100 \tTraining Loss: 0.017821 \tValidation Loss: 0.019672\n",
      "Training Precision: 0.97, Recall: 0.73, F1 Score: 0.83\n",
      "Validation Precision: 0.97, Recall: 0.64, F1 Score: 0.77\n",
      "Epoch 16/100 \tTraining Loss: 0.017351 \tValidation Loss: 0.018138\n",
      "Training Precision: 0.98, Recall: 0.74, F1 Score: 0.84\n",
      "Validation Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Epoch 17/100 \tTraining Loss: 0.014949 \tValidation Loss: 0.016376\n",
      "Training Precision: 0.98, Recall: 0.75, F1 Score: 0.85\n",
      "Validation Precision: 0.99, Recall: 0.74, F1 Score: 0.85\n",
      "Epoch 18/100 \tTraining Loss: 0.015934 \tValidation Loss: 0.016540\n",
      "Training Precision: 0.98, Recall: 0.75, F1 Score: 0.85\n",
      "Validation Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Epoch 19/100 \tTraining Loss: 0.013709 \tValidation Loss: 0.019588\n",
      "Training Precision: 0.98, Recall: 0.80, F1 Score: 0.88\n",
      "Validation Precision: 0.98, Recall: 0.72, F1 Score: 0.83\n",
      "Epoch 20/100 \tTraining Loss: 0.015632 \tValidation Loss: 0.018283\n",
      "Training Precision: 0.98, Recall: 0.79, F1 Score: 0.87\n",
      "Validation Precision: 0.98, Recall: 0.71, F1 Score: 0.82\n",
      "Epoch 21/100 \tTraining Loss: 0.015833 \tValidation Loss: 0.017629\n",
      "Training Precision: 0.98, Recall: 0.76, F1 Score: 0.86\n",
      "Validation Precision: 0.98, Recall: 0.72, F1 Score: 0.83\n",
      "Epoch 22/100 \tTraining Loss: 0.013755 \tValidation Loss: 0.018718\n",
      "Training Precision: 0.98, Recall: 0.78, F1 Score: 0.87\n",
      "Validation Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Epoch 23/100 \tTraining Loss: 0.015389 \tValidation Loss: 0.020259\n",
      "Training Precision: 0.97, Recall: 0.75, F1 Score: 0.85\n",
      "Validation Precision: 0.95, Recall: 0.75, F1 Score: 0.84\n",
      "Epoch 24/100 \tTraining Loss: 0.012915 \tValidation Loss: 0.018579\n",
      "Training Precision: 0.99, Recall: 0.79, F1 Score: 0.88\n",
      "Validation Precision: 0.98, Recall: 0.74, F1 Score: 0.84\n",
      "Epoch 25/100 \tTraining Loss: 0.014106 \tValidation Loss: 0.016276\n",
      "Training Precision: 0.98, Recall: 0.81, F1 Score: 0.89\n",
      "Validation Precision: 0.97, Recall: 0.72, F1 Score: 0.82\n",
      "Epoch 26/100 \tTraining Loss: 0.012861 \tValidation Loss: 0.018860\n",
      "Training Precision: 0.99, Recall: 0.80, F1 Score: 0.88\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.87\n",
      "Epoch 27/100 \tTraining Loss: 0.013703 \tValidation Loss: 0.021224\n",
      "Training Precision: 0.98, Recall: 0.79, F1 Score: 0.88\n",
      "Validation Precision: 0.97, Recall: 0.67, F1 Score: 0.79\n",
      "Epoch 28/100 \tTraining Loss: 0.012343 \tValidation Loss: 0.017895\n",
      "Training Precision: 0.98, Recall: 0.81, F1 Score: 0.89\n",
      "Validation Precision: 0.97, Recall: 0.74, F1 Score: 0.84\n",
      "Epoch 29/100 \tTraining Loss: 0.013611 \tValidation Loss: 0.017930\n",
      "Training Precision: 0.99, Recall: 0.80, F1 Score: 0.88\n",
      "Validation Precision: 0.97, Recall: 0.72, F1 Score: 0.82\n",
      "Epoch 30/100 \tTraining Loss: 0.011921 \tValidation Loss: 0.020292\n",
      "Training Precision: 0.98, Recall: 0.83, F1 Score: 0.90\n",
      "Validation Precision: 0.95, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 31/100 \tTraining Loss: 0.010156 \tValidation Loss: 0.019151\n",
      "Training Precision: 0.99, Recall: 0.84, F1 Score: 0.91\n",
      "Validation Precision: 0.96, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 32/100 \tTraining Loss: 0.010640 \tValidation Loss: 0.020698\n",
      "Training Precision: 0.99, Recall: 0.86, F1 Score: 0.92\n",
      "Validation Precision: 0.97, Recall: 0.76, F1 Score: 0.85\n",
      "Epoch 33/100 \tTraining Loss: 0.013457 \tValidation Loss: 0.018042\n",
      "Training Precision: 0.97, Recall: 0.78, F1 Score: 0.87\n",
      "Validation Precision: 0.97, Recall: 0.80, F1 Score: 0.88\n",
      "Epoch 34/100 \tTraining Loss: 0.010286 \tValidation Loss: 0.020374\n",
      "Training Precision: 0.99, Recall: 0.85, F1 Score: 0.92\n",
      "Validation Precision: 0.95, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 35/100 \tTraining Loss: 0.010398 \tValidation Loss: 0.018223\n",
      "Training Precision: 0.99, Recall: 0.85, F1 Score: 0.91\n",
      "Validation Precision: 0.97, Recall: 0.77, F1 Score: 0.86\n",
      "Epoch 36/100 \tTraining Loss: 0.009515 \tValidation Loss: 0.020388\n",
      "Training Precision: 0.99, Recall: 0.85, F1 Score: 0.92\n",
      "Validation Precision: 0.96, Recall: 0.73, F1 Score: 0.83\n",
      "Epoch 37/100 \tTraining Loss: 0.009356 \tValidation Loss: 0.022063\n",
      "Training Precision: 0.99, Recall: 0.85, F1 Score: 0.92\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.88\n",
      "Epoch 38/100 \tTraining Loss: 0.008438 \tValidation Loss: 0.023838\n",
      "Training Precision: 0.99, Recall: 0.88, F1 Score: 0.93\n",
      "Validation Precision: 0.94, Recall: 0.81, F1 Score: 0.87\n",
      "Epoch 39/100 \tTraining Loss: 0.008104 \tValidation Loss: 0.019240\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.98, Recall: 0.78, F1 Score: 0.87\n",
      "Epoch 40/100 \tTraining Loss: 0.009358 \tValidation Loss: 0.025292\n",
      "Training Precision: 0.99, Recall: 0.86, F1 Score: 0.92\n",
      "Validation Precision: 0.98, Recall: 0.67, F1 Score: 0.80\n",
      "Epoch 41/100 \tTraining Loss: 0.010281 \tValidation Loss: 0.020202\n",
      "Training Precision: 0.99, Recall: 0.86, F1 Score: 0.92\n",
      "Validation Precision: 0.96, Recall: 0.73, F1 Score: 0.83\n",
      "Epoch 42/100 \tTraining Loss: 0.008210 \tValidation Loss: 0.020941\n",
      "Training Precision: 0.99, Recall: 0.88, F1 Score: 0.93\n",
      "Validation Precision: 0.97, Recall: 0.75, F1 Score: 0.85\n",
      "Epoch 43/100 \tTraining Loss: 0.009632 \tValidation Loss: 0.020811\n",
      "Training Precision: 0.99, Recall: 0.86, F1 Score: 0.92\n",
      "Validation Precision: 0.95, Recall: 0.79, F1 Score: 0.87\n",
      "Epoch 44/100 \tTraining Loss: 0.009644 \tValidation Loss: 0.022684\n",
      "Training Precision: 0.99, Recall: 0.87, F1 Score: 0.93\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 45/100 \tTraining Loss: 0.009270 \tValidation Loss: 0.023259\n",
      "Training Precision: 0.99, Recall: 0.88, F1 Score: 0.93\n",
      "Validation Precision: 0.98, Recall: 0.70, F1 Score: 0.82\n",
      "Epoch 46/100 \tTraining Loss: 0.008077 \tValidation Loss: 0.022388\n",
      "Training Precision: 0.99, Recall: 0.87, F1 Score: 0.93\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 47/100 \tTraining Loss: 0.007953 \tValidation Loss: 0.021128\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.98, Recall: 0.77, F1 Score: 0.86\n",
      "Epoch 48/100 \tTraining Loss: 0.008425 \tValidation Loss: 0.020154\n",
      "Training Precision: 0.99, Recall: 0.87, F1 Score: 0.92\n",
      "Validation Precision: 0.97, Recall: 0.77, F1 Score: 0.86\n",
      "Epoch 49/100 \tTraining Loss: 0.006781 \tValidation Loss: 0.023282\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.98, Recall: 0.75, F1 Score: 0.85\n",
      "Epoch 50/100 \tTraining Loss: 0.007935 \tValidation Loss: 0.023162\n",
      "Training Precision: 0.99, Recall: 0.89, F1 Score: 0.94\n",
      "Validation Precision: 0.96, Recall: 0.75, F1 Score: 0.85\n",
      "Epoch 51/100 \tTraining Loss: 0.006107 \tValidation Loss: 0.021135\n",
      "Training Precision: 0.99, Recall: 0.89, F1 Score: 0.94\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.89\n",
      "Epoch 52/100 \tTraining Loss: 0.006422 \tValidation Loss: 0.021710\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.95, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 53/100 \tTraining Loss: 0.008863 \tValidation Loss: 0.024278\n",
      "Training Precision: 0.99, Recall: 0.87, F1 Score: 0.93\n",
      "Validation Precision: 0.97, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 54/100 \tTraining Loss: 0.008760 \tValidation Loss: 0.020077\n",
      "Training Precision: 0.99, Recall: 0.87, F1 Score: 0.93\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 55/100 \tTraining Loss: 0.006054 \tValidation Loss: 0.023327\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.84, F1 Score: 0.90\n",
      "Epoch 56/100 \tTraining Loss: 0.006192 \tValidation Loss: 0.021468\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.98, Recall: 0.75, F1 Score: 0.85\n",
      "Epoch 57/100 \tTraining Loss: 0.007637 \tValidation Loss: 0.022584\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.77, F1 Score: 0.86\n",
      "Epoch 58/100 \tTraining Loss: 0.008783 \tValidation Loss: 0.019731\n",
      "Training Precision: 0.99, Recall: 0.85, F1 Score: 0.92\n",
      "Validation Precision: 0.96, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 59/100 \tTraining Loss: 0.007538 \tValidation Loss: 0.022174\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 60/100 \tTraining Loss: 0.006763 \tValidation Loss: 0.021825\n",
      "Training Precision: 1.00, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.97, Recall: 0.80, F1 Score: 0.88\n",
      "Epoch 61/100 \tTraining Loss: 0.005211 \tValidation Loss: 0.026588\n",
      "Training Precision: 1.00, Recall: 0.92, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.85, F1 Score: 0.90\n",
      "Epoch 62/100 \tTraining Loss: 0.006931 \tValidation Loss: 0.024072\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.76, F1 Score: 0.85\n",
      "Epoch 63/100 \tTraining Loss: 0.005742 \tValidation Loss: 0.022551\n",
      "Training Precision: 1.00, Recall: 0.90, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.80, F1 Score: 0.88\n",
      "Epoch 64/100 \tTraining Loss: 0.007363 \tValidation Loss: 0.024938\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.96, Recall: 0.86, F1 Score: 0.91\n",
      "Epoch 65/100 \tTraining Loss: 0.006514 \tValidation Loss: 0.022787\n",
      "Training Precision: 0.99, Recall: 0.92, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.78, F1 Score: 0.87\n",
      "Epoch 66/100 \tTraining Loss: 0.006419 \tValidation Loss: 0.026232\n",
      "Training Precision: 0.99, Recall: 0.92, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.74, F1 Score: 0.84\n",
      "Epoch 67/100 \tTraining Loss: 0.007201 \tValidation Loss: 0.025474\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.97, Recall: 0.76, F1 Score: 0.85\n",
      "Epoch 68/100 \tTraining Loss: 0.005615 \tValidation Loss: 0.026300\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.81, F1 Score: 0.89\n",
      "Epoch 69/100 \tTraining Loss: 0.005311 \tValidation Loss: 0.027284\n",
      "Training Precision: 1.00, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 70/100 \tTraining Loss: 0.005657 \tValidation Loss: 0.020436\n",
      "Training Precision: 0.99, Recall: 0.92, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.80, F1 Score: 0.88\n",
      "Epoch 71/100 \tTraining Loss: 0.006934 \tValidation Loss: 0.023328\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.94\n",
      "Validation Precision: 0.96, Recall: 0.80, F1 Score: 0.87\n",
      "Epoch 72/100 \tTraining Loss: 0.003591 \tValidation Loss: 0.029038\n",
      "Training Precision: 1.00, Recall: 0.94, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.89\n",
      "Epoch 73/100 \tTraining Loss: 0.004675 \tValidation Loss: 0.029095\n",
      "Training Precision: 0.99, Recall: 0.92, F1 Score: 0.96\n",
      "Validation Precision: 0.95, Recall: 0.81, F1 Score: 0.87\n",
      "Epoch 74/100 \tTraining Loss: 0.007177 \tValidation Loss: 0.029101\n",
      "Training Precision: 0.99, Recall: 0.88, F1 Score: 0.93\n",
      "Validation Precision: 0.96, Recall: 0.74, F1 Score: 0.84\n",
      "Epoch 75/100 \tTraining Loss: 0.006934 \tValidation Loss: 0.026212\n",
      "Training Precision: 0.99, Recall: 0.88, F1 Score: 0.93\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.89\n",
      "Epoch 76/100 \tTraining Loss: 0.004977 \tValidation Loss: 0.024440\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.78, F1 Score: 0.87\n",
      "Epoch 77/100 \tTraining Loss: 0.006280 \tValidation Loss: 0.024606\n",
      "Training Precision: 1.00, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.96, Recall: 0.80, F1 Score: 0.87\n",
      "Epoch 78/100 \tTraining Loss: 0.003358 \tValidation Loss: 0.026404\n",
      "Training Precision: 1.00, Recall: 0.96, F1 Score: 0.98\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 79/100 \tTraining Loss: 0.003817 \tValidation Loss: 0.028142\n",
      "Training Precision: 1.00, Recall: 0.93, F1 Score: 0.97\n",
      "Validation Precision: 0.97, Recall: 0.84, F1 Score: 0.90\n",
      "Epoch 80/100 \tTraining Loss: 0.004711 \tValidation Loss: 0.029140\n",
      "Training Precision: 1.00, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.81, F1 Score: 0.89\n",
      "Epoch 81/100 \tTraining Loss: 0.007777 \tValidation Loss: 0.024558\n",
      "Training Precision: 0.99, Recall: 0.91, F1 Score: 0.95\n",
      "Validation Precision: 0.97, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 82/100 \tTraining Loss: 0.006659 \tValidation Loss: 0.027348\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.96, Recall: 0.78, F1 Score: 0.86\n",
      "Epoch 83/100 \tTraining Loss: 0.004485 \tValidation Loss: 0.028214\n",
      "Training Precision: 1.00, Recall: 0.94, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.85, F1 Score: 0.90\n",
      "Epoch 84/100 \tTraining Loss: 0.004277 \tValidation Loss: 0.028298\n",
      "Training Precision: 1.00, Recall: 0.95, F1 Score: 0.97\n",
      "Validation Precision: 0.97, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 85/100 \tTraining Loss: 0.004286 \tValidation Loss: 0.029867\n",
      "Training Precision: 1.00, Recall: 0.93, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 86/100 \tTraining Loss: 0.003408 \tValidation Loss: 0.031588\n",
      "Training Precision: 1.00, Recall: 0.95, F1 Score: 0.97\n",
      "Validation Precision: 0.97, Recall: 0.82, F1 Score: 0.89\n",
      "Epoch 87/100 \tTraining Loss: 0.004336 \tValidation Loss: 0.027709\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.84, F1 Score: 0.90\n",
      "Epoch 88/100 \tTraining Loss: 0.003551 \tValidation Loss: 0.032067\n",
      "Training Precision: 0.99, Recall: 0.94, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.81, F1 Score: 0.87\n",
      "Epoch 89/100 \tTraining Loss: 0.004883 \tValidation Loss: 0.028617\n",
      "Training Precision: 1.00, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.96, Recall: 0.79, F1 Score: 0.87\n",
      "Epoch 90/100 \tTraining Loss: 0.004979 \tValidation Loss: 0.029401\n",
      "Training Precision: 0.99, Recall: 0.94, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.81, F1 Score: 0.88\n",
      "Epoch 91/100 \tTraining Loss: 0.004548 \tValidation Loss: 0.027642\n",
      "Training Precision: 1.00, Recall: 0.95, F1 Score: 0.97\n",
      "Validation Precision: 0.97, Recall: 0.83, F1 Score: 0.90\n",
      "Epoch 92/100 \tTraining Loss: 0.004855 \tValidation Loss: 0.033322\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.95, Recall: 0.84, F1 Score: 0.89\n",
      "Epoch 93/100 \tTraining Loss: 0.003051 \tValidation Loss: 0.033964\n",
      "Training Precision: 1.00, Recall: 0.96, F1 Score: 0.98\n",
      "Validation Precision: 0.94, Recall: 0.83, F1 Score: 0.88\n",
      "Epoch 94/100 \tTraining Loss: 0.004379 \tValidation Loss: 0.029825\n",
      "Training Precision: 0.99, Recall: 0.95, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.80, F1 Score: 0.87\n",
      "Epoch 95/100 \tTraining Loss: 0.003020 \tValidation Loss: 0.030447\n",
      "Training Precision: 1.00, Recall: 0.96, F1 Score: 0.98\n",
      "Validation Precision: 0.97, Recall: 0.84, F1 Score: 0.90\n",
      "Epoch 96/100 \tTraining Loss: 0.003848 \tValidation Loss: 0.029329\n",
      "Training Precision: 1.00, Recall: 0.94, F1 Score: 0.97\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.89\n",
      "Epoch 97/100 \tTraining Loss: 0.003306 \tValidation Loss: 0.027915\n",
      "Training Precision: 1.00, Recall: 0.96, F1 Score: 0.98\n",
      "Validation Precision: 0.96, Recall: 0.82, F1 Score: 0.88\n",
      "Epoch 98/100 \tTraining Loss: 0.005422 \tValidation Loss: 0.023657\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.96, Recall: 0.80, F1 Score: 0.87\n",
      "Epoch 99/100 \tTraining Loss: 0.005631 \tValidation Loss: 0.033648\n",
      "Training Precision: 0.99, Recall: 0.93, F1 Score: 0.96\n",
      "Validation Precision: 0.97, Recall: 0.83, F1 Score: 0.90\n",
      "Epoch 100/100 \tTraining Loss: 0.007542 \tValidation Loss: 0.041684\n",
      "Training Precision: 0.99, Recall: 0.90, F1 Score: 0.95\n",
      "Validation Precision: 0.92, Recall: 0.86, F1 Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load and preprocess your data\n",
    "# data = pd.read_csv('your_data.csv')  # Load your data\n",
    "# Perform necessary preprocessing steps\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('Abandono', axis=1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, data['Abandono'], test_size=0.3, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Neural Network Definition\n",
    "class EnhancedNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EnhancedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "model = EnhancedNet(input_size=X_train.shape[1]).to(device)\n",
    "\n",
    "# Adjust class weights for binary classification\n",
    "class_counts = y_train.value_counts()\n",
    "class_weights = [class_counts[0] / class_counts[1], class_counts[1] / class_counts[0]]\n",
    "weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights_tensor[1])  # Use the weight for the positive class\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)    \n",
    "\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience, trials = 100, 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, all_train_labels, all_train_preds = 0.0, [], []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        preds = torch.sigmoid(outputs).detach().cpu().numpy() > 0.5\n",
    "        all_train_preds.extend(preds)\n",
    "        all_train_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, all_val_labels, all_val_preds = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = torch.sigmoid(outputs).detach().cpu().numpy() > 0.5\n",
    "            all_val_preds.extend(preds)\n",
    "            all_val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate Loss and Metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_train_labels, all_train_preds, average='binary')\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {val_loss:.6f}')\n",
    "    print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1 Score: {train_f1:.2f}')\n",
    "    print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trials = 0\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a551715f-21c6-4a03-b402-21ce706c4710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "Epoch 1/200 \tTraining Loss: 0.083675 \tValidation Loss: 0.035687\n",
      "Optimal Threshold: 0.05\n",
      "Training Precision: 0.49, Recall: 0.03, F1 Score: 0.06\n",
      "Validation Precision: 0.67, Recall: 0.93, F1 Score: 0.78\n",
      "Epoch 11/200 \tTraining Loss: 0.014097 \tValidation Loss: 0.029093\n",
      "Optimal Threshold: 0.11\n",
      "Training Precision: 0.91, Recall: 0.93, F1 Score: 0.92\n",
      "Validation Precision: 0.81, Recall: 0.90, F1 Score: 0.85\n",
      "Epoch 21/200 \tTraining Loss: 0.010477 \tValidation Loss: 0.027346\n",
      "Optimal Threshold: 0.13\n",
      "Training Precision: 0.93, Recall: 0.95, F1 Score: 0.94\n",
      "Validation Precision: 0.87, Recall: 0.88, F1 Score: 0.87\n",
      "Epoch 31/200 \tTraining Loss: 0.004472 \tValidation Loss: 0.054533\n",
      "Optimal Threshold: 0.20\n",
      "Training Precision: 0.99, Recall: 0.98, F1 Score: 0.99\n",
      "Validation Precision: 0.88, Recall: 0.90, F1 Score: 0.89\n",
      "Epoch 41/200 \tTraining Loss: 0.003122 \tValidation Loss: 0.054440\n",
      "Optimal Threshold: 0.12\n",
      "Training Precision: 0.97, Recall: 0.99, F1 Score: 0.98\n",
      "Validation Precision: 0.87, Recall: 0.87, F1 Score: 0.87\n",
      "Epoch 51/200 \tTraining Loss: 0.003353 \tValidation Loss: 0.053642\n",
      "Optimal Threshold: 0.09\n",
      "Training Precision: 0.99, Recall: 0.98, F1 Score: 0.99\n",
      "Validation Precision: 0.90, Recall: 0.84, F1 Score: 0.87\n",
      "Epoch 61/200 \tTraining Loss: 0.002123 \tValidation Loss: 0.055800\n",
      "Optimal Threshold: 0.13\n",
      "Training Precision: 0.99, Recall: 0.99, F1 Score: 0.99\n",
      "Validation Precision: 0.93, Recall: 0.83, F1 Score: 0.88\n",
      "Epoch 71/200 \tTraining Loss: 0.006061 \tValidation Loss: 0.044016\n",
      "Optimal Threshold: 0.22\n",
      "Training Precision: 0.97, Recall: 0.96, F1 Score: 0.97\n",
      "Validation Precision: 0.90, Recall: 0.85, F1 Score: 0.87\n",
      "Epoch 81/200 \tTraining Loss: 0.000857 \tValidation Loss: 0.060897\n",
      "Optimal Threshold: 0.05\n",
      "Training Precision: 1.00, Recall: 0.99, F1 Score: 1.00\n",
      "Validation Precision: 0.91, Recall: 0.90, F1 Score: 0.90\n",
      "Epoch 91/200 \tTraining Loss: 0.004548 \tValidation Loss: 0.048413\n",
      "Optimal Threshold: 0.14\n",
      "Training Precision: 0.99, Recall: 0.97, F1 Score: 0.98\n",
      "Validation Precision: 0.90, Recall: 0.85, F1 Score: 0.87\n",
      "Epoch 101/200 \tTraining Loss: 0.000621 \tValidation Loss: 0.065622\n",
      "Optimal Threshold: 0.05\n",
      "Training Precision: 1.00, Recall: 0.99, F1 Score: 1.00\n",
      "Validation Precision: 0.89, Recall: 0.89, F1 Score: 0.89\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess your data\n",
    "# data = pd.read_csv('your_data.csv')  # Load your data\n",
    "# Perform necessary preprocessing steps\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('Abandono', axis=1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, data['Abandono'], test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Neural Network Definition\n",
    "class EnhancedNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EnhancedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2048)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "model = EnhancedNet(input_size=X_train.shape[1]).to(device)\n",
    "\n",
    "# Adjust class weights for binary classification\n",
    "class_counts = y_train.value_counts()\n",
    "class_weights = [class_counts[0] / class_counts[1], class_counts[1] / class_counts[0]]\n",
    "weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights_tensor[1])  # Use the weight for the positive class\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)    \n",
    "\n",
    "# Function to calculate the optimal threshold\n",
    "def find_optimal_threshold(y_true, y_scores):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    ix = np.argmax(fscore)\n",
    "    return thresholds[ix]\n",
    "\n",
    "# Training Loop with Early Stopping and Dynamic Threshold\n",
    "epochs = 200\n",
    "best_val_f1 = -float('inf')\n",
    "patience, trials = 100, 0\n",
    "optimal_threshold = 0.5  # Initial threshold\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, all_train_labels, all_train_preds, all_train_scores = 0.0, [], [], []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        scores = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        all_train_scores.extend(scores)\n",
    "        preds = scores > optimal_threshold\n",
    "        all_train_preds.extend(preds)\n",
    "        all_train_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate optimal threshold after training phase\n",
    "    if epoch % 5 == 0:\n",
    "        optimal_threshold = find_optimal_threshold(all_train_labels, np.array(all_train_scores))\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, all_val_labels, all_val_preds, all_val_scores = 0.0, [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            scores = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            all_val_scores.extend(scores)\n",
    "            preds = scores > optimal_threshold\n",
    "            all_val_preds.extend(preds)\n",
    "            all_val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate Loss and Metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(all_train_labels, all_train_preds, average='binary')\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {val_loss:.6f}')\n",
    "        print(f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "        print(f'Training Precision: {train_precision:.2f}, Recall: {train_recall:.2f}, F1 Score: {train_f1:.2f}')\n",
    "        print(f'Validation Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        trials = 0\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "model_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02fb02b-f22c-420b-8016-cd1d09aa8934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedNet(\n",
       "  (fc1): Linear(in_features=33, out_features=2048, bias=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = EnhancedNet(input_size=X_train.shape[1]).to(device)\n",
    "\n",
    "\n",
    "model_path = \"model.pth\"\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6835c2-869a-428e-a84e-dcac667fea3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- CNA\n- Ensino Basico Mae\n- Ensino Basico Pai\n- Ensino Outros Mae\n- Ensino Outros Pai\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         max_val \u001b[38;5;241m=\u001b[39m data[column]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     11\u001b[0m         simulated_data[column] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(min_val, max_val \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_samples)\n\u001b[0;32m---> 13\u001b[0m scaled_features_new_data \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m scaled_features \u001b[38;5;241m=\u001b[39m scaled_features\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbandono\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
      "File \u001b[0;32m~/dev/AI-2/venv/lib/python3.11/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- CNA\n- Ensino Basico Mae\n- Ensino Basico Pai\n- Ensino Outros Mae\n- Ensino Outros Pai\n- ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1000  # Number of samples in the new dataset\n",
    "simulated_data = pd.DataFrame()\n",
    "\n",
    "for column in data.columns:\n",
    "    if column not in ['Unnamed: 0', 'Abandono']:\n",
    "        min_val = data[column].min()\n",
    "        max_val = data[column].max()\n",
    "        simulated_data[column] = np.random.randint(min_val, max_val + 1, num_samples)\n",
    "\n",
    "scaled_features_new_data = scaler.transform(new_data)\n",
    "\n",
    "scaled_features = scaled_features.drop(['Abandono', 'Unnamed: 0'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
